{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 3497,
     "status": "ok",
     "timestamp": 1650316229575,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "_2tiyc-Tor_E",
    "ExecuteTime": {
     "end_time": "2025-08-24T14:17:01.963269Z",
     "start_time": "2025-08-24T14:16:57.876451Z"
    }
   },
   "source": [
    "##SciBERT: https://github.com/allenai/scibert\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizerFast, BertModel, BertTokenizer\n",
    "import os\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T14:17:01.980269Z",
     "start_time": "2025-08-24T14:17:01.967454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.version.cuda)  # Should print a version if CUDA-enabled\n",
    "print(torch.cuda.is_available())  # Should be True"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.8\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1650316234060,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "DYCsubKxpKxs",
    "outputId": "88153fa4-fac8-40b4-ac1c-a059b317b4e4",
    "ExecuteTime": {
     "end_time": "2025-08-24T14:17:01.998862Z",
     "start_time": "2025-08-24T14:17:01.993932Z"
    }
   },
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 5090\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgHvpM9czGv_"
   },
   "source": [
    "# Making bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7418,
     "status": "ok",
     "timestamp": 1650316241474,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "Xv_CLm0xpXMw",
    "outputId": "68943eef-ee36-432e-c15e-8b326b8b67e0",
    "ExecuteTime": {
     "end_time": "2025-08-24T14:18:07.336325Z",
     "start_time": "2025-08-24T14:18:01.817022Z"
    }
   },
   "source": [
    "##Use SciBERT instaed of BERT\n",
    "# import SciBERT-base pretrained model\n",
    "\n",
    "# Set HuggingFace cache path\n",
    "\n",
    "cache_path = r\"F:\\ai_playground\"\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name, cache_dir=cache_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_path)\n",
    "\n",
    "\n",
    "\n",
    "##BERT\n",
    "# bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95dc1a6c755d444b9cfd556e02263972"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\One Drives\\OneDrive - Nanyang Technological University\\Academics\\!win_dev\\mae_fyp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in F:\\ai_playground\\models--allenai--scibert_scivocab_uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee41276a6fd546c0a5abdeedb22b803e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5fe52bbd25044d12a4beb7a36baf67b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15dc0bdf2c724b178190f912f8c14c4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4082,
     "status": "ok",
     "timestamp": 1650316245512,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "1q2JkE1trRf1",
    "outputId": "cacc2b87-4c89-4b3e-8c07-fd83a652670a",
    "ExecuteTime": {
     "end_time": "2025-08-24T14:18:19.250886Z",
     "start_time": "2025-08-24T14:18:17.226453Z"
    }
   },
   "source": [
    "from src.file_handling import file_location\n",
    "\n",
    "folder_path = file_location.FileLocation()\n",
    "root_path = folder_path.root\n",
    "data_path = root_path.parent.parent / 'fyp' / 'data'\n",
    "\n",
    "asme_path = file_location.FolderPathOfASME(data_path)\n",
    "jmd_all_updated_cleaned_excel_path = asme_path.asme_jmd / 'jmd_all_updated cleaned.xlsx'\n",
    "\n",
    "orig_data = pd.read_excel(jmd_all_updated_cleaned_excel_path)\n",
    "print('Number of articles: {:,}\\n'.format(orig_data.shape[0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 5,591\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650316247928,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "8ZsIdrz6eBzW",
    "ExecuteTime": {
     "end_time": "2025-08-24T14:18:20.360211Z",
     "start_time": "2025-08-24T14:18:20.310739Z"
    }
   },
   "source": [
    "## exclude a part of papers according to issue_section info\n",
    "remove_set = [\"Issue Section:\\nLetters to the Editor\", \"Issue Section:\\nEditorial\", \"Issue Section:\\nIn Memoriam\", \n",
    "              \"Issue Section:\\nDiscussions\", \"Issue Section:\\nErrata\", \"Issue Section:\\nBook Reviews\",\n",
    "              \"Issue Section:\\nReports\", \"Issue Section:\\nGuest Editorial\", \"Issue Section:\\nAnnouncements\", \n",
    "              \"Issue Section:\\nCommentary\", \"Issue Section:\\nRetrospectives\", \"Issue Section:\\nBook Review\",\n",
    "              \"Issue Section:\\nObituary\", \"Issue Section:\\nErratum\", \"Issue Section:\\nEditor's Note\", \n",
    "              \"Issue Section:\\nIntroduction\", \"Issue Section:\\nEditorials\", \"Issue Section:\\nDiscussion\", \n",
    "              \"Issue Section:\\nSurvey\", \"Issue Section:\\nAnnouncement\"]\n",
    "\n",
    "data = orig_data[~orig_data['issue_section'].isin(remove_set)]\n",
    "len(data)\n",
    "\n",
    "##munually removed a few papers after this"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5121"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-08-24T14:17:06.447477Z",
     "start_time": "2025-08-24T14:17:06.244852Z"
    }
   },
   "source": [
    "##reload data\n",
    "datax = pd.read_excel('data with cluster.xlsx') \n",
    "data = orig_data[orig_data['doi'].isin(list(datax['doi']))]\n",
    "\n",
    "'''\n",
    "Manually clean the text data: integrate the results from 2 pdf2text approaches and put the main-body text in columns:\n",
    "['text 0', 'text 1', 'text 2']\n",
    "'''\n",
    "data = data.astype({'title':str, 'abstract':str, 'keywords':str, 'text 0':str, 'text 1':str, 'text 2':str})\n",
    "info = {}\n",
    "info['lst1'] = list(data['title'])\n",
    "info['lst2'] = list(data['abstract'])\n",
    "info['lst3'] = list(data['keywords'])\n",
    "info['lst4'] = list(data['text 0'])\n",
    "info['lst5'] = list(data['text 1'])\n",
    "info['lst6'] = list(data['text 2'])\n",
    "\n",
    "##use title and abstract for analysis\n",
    "# var = 'TA'\n",
    "##use title, abstract, and main-body text for analysis\n",
    "var = 'FULL'\n",
    "\n",
    "if var == 'TA':\n",
    "    n = 2\n",
    "else:\n",
    "    n = 6\n",
    "    \n",
    "text = []\n",
    "for i in range(0,len(data)):\n",
    "    txt = [info[key][i] for key in list(info.keys())[0:n]]\n",
    "    ##clean keywords\n",
    "    txt[2] = txt[2].replace('Keywords:\\n', '').replace('\\n', '')\n",
    "    ##join the elements\n",
    "    txt1 = ' '.join(txt[0:3]).strip()\n",
    "    txt2 = ''.join(txt[3:6]).strip()\n",
    "    txt3 = ' '.join([txt1, txt2]).strip()\n",
    "    text.append(txt3)\n",
    "    \n",
    "data['text'] = text\n",
    "data = data[data.text.notnull()]\n",
    "len(data)"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data with cluster.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m##reload data\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m datax = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_excel\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdata with cluster.xlsx\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m \n\u001B[32m      3\u001B[39m data = orig_data[orig_data[\u001B[33m'\u001B[39m\u001B[33mdoi\u001B[39m\u001B[33m'\u001B[39m].isin(\u001B[38;5;28mlist\u001B[39m(datax[\u001B[33m'\u001B[39m\u001B[33mdoi\u001B[39m\u001B[33m'\u001B[39m]))]\n\u001B[32m      5\u001B[39m \u001B[33;03m'''\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[33;03mManually clean the text data: integrate the results from 2 pdf2text approaches and put the main-body text in columns:\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[33;03m['text 0', 'text 1', 'text 2']\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[33;03m'''\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\One Drives\\OneDrive - Nanyang Technological University\\Academics\\!win_dev\\mae_fyp\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001B[39m, in \u001B[36mread_excel\u001B[39m\u001B[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001B[39m\n\u001B[32m    493\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(io, ExcelFile):\n\u001B[32m    494\u001B[39m     should_close = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     io = \u001B[43mExcelFile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    496\u001B[39m \u001B[43m        \u001B[49m\u001B[43mio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    498\u001B[39m \u001B[43m        \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m=\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    499\u001B[39m \u001B[43m        \u001B[49m\u001B[43mengine_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mengine_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    500\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    501\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m engine \u001B[38;5;129;01mand\u001B[39;00m engine != io.engine:\n\u001B[32m    502\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    503\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mEngine should not be specified when passing \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    504\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33man ExcelFile - ExcelFile already has the engine set\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    505\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\One Drives\\OneDrive - Nanyang Technological University\\Academics\\!win_dev\\mae_fyp\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001B[39m, in \u001B[36mExcelFile.__init__\u001B[39m\u001B[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001B[39m\n\u001B[32m   1548\u001B[39m     ext = \u001B[33m\"\u001B[39m\u001B[33mxls\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1549\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1550\u001B[39m     ext = \u001B[43minspect_excel_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1551\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_options\u001B[49m\n\u001B[32m   1552\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1553\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ext \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1554\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1555\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mExcel file format cannot be determined, you must specify \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1556\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33man engine manually.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1557\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\One Drives\\OneDrive - Nanyang Technological University\\Academics\\!win_dev\\mae_fyp\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001B[39m, in \u001B[36minspect_excel_format\u001B[39m\u001B[34m(content_or_path, storage_options)\u001B[39m\n\u001B[32m   1399\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(content_or_path, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[32m   1400\u001B[39m     content_or_path = BytesIO(content_or_path)\n\u001B[32m-> \u001B[39m\u001B[32m1402\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1403\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[32m   1404\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handle:\n\u001B[32m   1405\u001B[39m     stream = handle.handle\n\u001B[32m   1406\u001B[39m     stream.seek(\u001B[32m0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\One Drives\\OneDrive - Nanyang Technological University\\Academics\\!win_dev\\mae_fyp\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    873\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(\n\u001B[32m    874\u001B[39m             handle,\n\u001B[32m    875\u001B[39m             ioargs.mode,\n\u001B[32m   (...)\u001B[39m\u001B[32m    878\u001B[39m             newline=\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    879\u001B[39m         )\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m882\u001B[39m         handle = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    883\u001B[39m     handles.append(handle)\n\u001B[32m    885\u001B[39m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'data with cluster.xlsx'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650316247929,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "gK2fwERDbDAg",
    "outputId": "59a3e010-6681-4fcb-8f8e-ee14026d6b08"
   },
   "source": [
    "##only keep the relevant columns\n",
    "data = data[['title', 'abstract', 'doi', 'year', 'issue',\n",
    "       'citation_info', 'authors', 'author affiliation', 'pdf_link',\n",
    "       'issue_section', 'topics', 'cited times', 'keywords', 'reference',\n",
    "       'related article', 'source of related articles', 'text 0', 'text 1', 'text 2']]\n",
    "\n",
    "print(len(data))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1650316247929,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "b0ozEwouTg6a"
   },
   "source": [
    "##split long text into multiple piece\n",
    "new_text = {}\n",
    "new_text['ID'] = []\n",
    "new_text['text'] = []\n",
    "m = 0\n",
    "for item in text:\n",
    "    txt = item.split()\n",
    "    n = int(len(text)/500) + 1\n",
    "    for i in range(0,n):\n",
    "        txt1 = txt[500*i:min(500*(i+1),len(txt))]\n",
    "        txt2 = ' '.join(txt1).strip()\n",
    "        new_text['ID'].append(m)\n",
    "        new_text['text'].append(txt2)\n",
    "    m += 1\n",
    "new_text = pd.DataFrame(new_text)\n",
    "\n",
    "new_text.to_excel('truncated text.xlsx')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1650316248432,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "UC0apY-WhM67",
    "outputId": "b5e090fa-ed5e-48e3-a9c1-2e4bae623673"
   },
   "source": [
    "text1 = list(new_text['text'])\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "#     train_text.tolist(),\n",
    "    text1,\n",
    "    max_length = 500,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in text1]\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650316271604,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "lg8i6P-kiNfZ"
   },
   "source": [
    "## convert lists to tensors\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1650316271605,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "9RjELTVLiBPr"
   },
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 64\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = SequentialSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1650316271606,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "bTzVAmgqilQa"
   },
   "source": [
    "device = torch.device(\"cuda\")\n",
    "bert = bert.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82555,
     "status": "ok",
     "timestamp": 1650316354144,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "TPN0eYcLinPu",
    "outputId": "caa05bd9-5ef2-49d9-d0d0-61706df29a7c"
   },
   "source": [
    "# iterate over batches\n",
    "\n",
    "cls_all = []\n",
    "for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "        print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    "\n",
    "    sent_id, mask = batch \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    cls = bert(sent_id, mask,return_dict=False)[1]\n",
    "    cls_all.append(cls)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1650316354145,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "plQGq-hskDNy"
   },
   "source": [
    "embedding = cls_all[0].cpu().detach().numpy()\n",
    "for i in cls_all[1:]:\n",
    "    embedding = np.concatenate((embedding, i.cpu().detach().numpy()), axis=0)\n",
    "    \n",
    "np.savetxt('embeddings of truncated texts', embedding)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "##integrate embeddings of multiple truncated texts into 1 as the final embeddings of the long text using attention\n",
    "\n",
    "lst = list(set(new_text['ID']))\n",
    "new_emb = np.zeros((len(data),768))\n",
    "for i in lst:\n",
    "    tmp = new_text[new_text['ID'] == i]\n",
    "    idx = list(tmp.index)\n",
    "    emb = embedding[idx]\n",
    "    wgt_sum = 0\n",
    "    wgt = [0]*len(emb)\n",
    "    for a in range(0,len(emb)):\n",
    "        for b in range(0,len(emb)):\n",
    "            wgt_sum += np.dot(emb[a], emb[b])\n",
    "            wgt[a] += np.dot(emb[a], emb[b])\n",
    "    wgt1 = [j/wgt_sum for j in wgt]\n",
    "    for a in range(0,len(emb)):\n",
    "        new_emb[i] += wgt1[a]*emb[a]\n",
    "        \n",
    "np.savetxt('embeddings of long texts', new_emb)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1650316354145,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "MmymJ-ETkgfy"
   },
   "source": [
    "data['emb'] = list(new_emb)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 628,
     "status": "ok",
     "timestamp": 1650316354769,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "py6HFF0HjF8A"
   },
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "\n",
    "def clustering_question(data,var,NUM_CLUSTERS = 8):\n",
    "\n",
    "    X = np.array(data[var].tolist())\n",
    "\n",
    "    kclusterer = KMeansClusterer(\n",
    "        NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,\n",
    "        repeats=25,avoid_empty_clusters=True)\n",
    "\n",
    "    assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "\n",
    "    data['cluster_'+str(NUM_CLUSTERS)] = pd.Series(assigned_clusters, index=data.index)\n",
    "    data['centroid'] = data['cluster_'+str(NUM_CLUSTERS)].apply(lambda x: kclusterer.means()[x])\n",
    "\n",
    "    return data, assigned_clusters"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1650316354769,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "q2DNrm6U-nye"
   },
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "def distance_from_centroid(row,var):\n",
    "    # type of emb and centroid is different, hence using tolist below\n",
    "    return distance_matrix([row[var]], [row['centroid'].tolist()])[0][0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1650316354770,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "0slU59vd0xz1"
   },
   "source": [
    "#clusters data using num_clusters \n",
    "def get_cluster_question(input, num_clusters):\n",
    "    new_cluster_data, new_cluster_id = clustering_question(input, var, num_clusters)\n",
    "    new_cluster_data['cluster_id'] = new_cluster_id\n",
    "    new_cluster_data['distance_from_centroid'] = new_cluster_data.apply(distance_from_centroid, axis=1)\n",
    "    return new_cluster_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLhfULk46-kP"
   },
   "source": [
    "# Find ideal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1650316376005,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "uMOAzwzg7Ejx"
   },
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import math\n",
    "import matplotlib.style as style\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.decomposition import PCA"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 1408,
     "status": "ok",
     "timestamp": 1649728443060,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "KzhuDiXQShFA",
    "outputId": "ebef04e5-1c25-4e9a-fe08-8d18cd6b1dc2"
   },
   "source": [
    "## find the optimal reduced dimensions based on explained variance\n",
    "n = 100\n",
    "pca_n = PCA(n_components=n)\n",
    "emb_pca = pca_n.fit(new_emb)\n",
    "exp_var_pca_n = pca_n.explained_variance_ratio_\n",
    "cumul_sum_eigenvals = np.cumsum(exp_var_pca_n)\n",
    "\n",
    "plt.bar(range(0,len(exp_var_pca_n)), exp_var_pca_n, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cumul_sum_eigenvals)), cumul_sum_eigenvals, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1649728443447,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "EFjWh4-yVDYs",
    "outputId": "80f2a873-5869-4c3a-cf43-461256e5baaa"
   },
   "source": [
    "no = next(idx for idx, value in enumerate(cumul_sum_eigenvals) if value > 0.9)\n",
    "\n",
    "'''\n",
    "9 for full-texts and 42 for TAs. That is full-texts can be represented by lower-dimention data.\n",
    "Full-texts are more similar to each other? More noise?\n",
    "'''\n",
    "print (\"Dimentions: \", no)\n",
    "print (\"Explained variance: \",cumul_sum_eigenvals[no])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650316380108,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "REROUuLsgrTk"
   },
   "source": [
    "##get low-dimensional embeddings\n",
    "pca = PCA(n_components=no)\n",
    "pca.fit(new_emb)\n",
    "x = pca.fit_transform(new_emb)\n",
    "\n",
    "data['emb_reduced'] = x.tolist()\n",
    "\n",
    "##get 2-dimensional embeddings for visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(new_emb)\n",
    "x = pca.fit_transform(new_emb)\n",
    "\n",
    "data['X'] = x[:,0]\n",
    "data['Y'] = x[:,1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KB2PqMzs80rr"
   },
   "source": [
    "##silhouette analysis: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "def find_opt_clusters_4(data, embedding, min_num, max_num):\n",
    "    stored_clusters = {}\n",
    "    range_n_clusters = list(range(min_num, max_num))\n",
    "    silh_avg_n_clusters = []\n",
    "  \n",
    "    for n_clusters in range_n_clusters:\n",
    "        fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "        # Run the kmeans algorithm using \"emb_reduced\" or \"emb\"\n",
    "        cluster_data, cluster_labels = clustering_question(data,'emb', n_clusters)\n",
    "        stored_clusters[n_clusters] = (cluster_data, cluster_labels)\n",
    "\n",
    "        # density & separation of formed clusters\n",
    "        silh_avg = silhouette_score(embedding, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silh_avg)\n",
    "        silh_avg_n_clusters.append(silh_avg)\n",
    "\n",
    "        centroids  = cluster_data['centroid']\n",
    "\n",
    "        # get silhouette\n",
    "\n",
    "        silhouette_vals = silhouette_samples(embedding,cluster_labels) #silhouette_vals\n",
    "\n",
    "        # silhouette plot\n",
    "        y_ticks = []\n",
    "        y_lower = y_upper = 0\n",
    "        for i,cluster in enumerate(np.unique(cluster_labels)):\n",
    "            cluster_silhouette_vals = silhouette_vals[cluster_labels ==cluster]\n",
    "            cluster_silhouette_vals.sort()\n",
    "            y_upper += len(cluster_silhouette_vals)\n",
    "\n",
    "            ax[0].barh(range(y_lower,y_upper),\n",
    "            cluster_silhouette_vals,height =1);\n",
    "            ax[0].text(-0.03,(y_lower+y_upper)/2,str(i+1))\n",
    "            y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "            # Get the average silhouette score \n",
    "            avg_score = np.mean(silhouette_vals)\n",
    "            ax[0].axvline(avg_score,linestyle ='--',\n",
    "            linewidth =2,color = 'green')\n",
    "            ax[0].set_yticks([])\n",
    "            ax[0].set_xlim([-0.1, 1])\n",
    "            ax[0].set_xlabel('Silhouette coefficient values')\n",
    "            ax[0].set_ylabel('Cluster labels')\n",
    "            ax[0].set_title('Silhouette plot for the various clusters');\n",
    "\n",
    "            # scatter plot of data colored with labels\n",
    "\n",
    "            ax[1].scatter(data['X'],\n",
    "            data['Y'] , c = cluster_labels);\n",
    "#             ax[1].scatter(centroids[:,0],centroids[:,1],\n",
    "#             marker = '*' , c= 'r',s =250);\n",
    "            ax[1].set_xlabel('Eruption time in mins')\n",
    "            ax[1].set_ylabel('Waiting time to next eruption')\n",
    "            ax[1].set_title('Visualization of clustered data', y=1.02)\n",
    "\n",
    "            plt.suptitle(f' Silhouette analysis using num_clusters = {n_clusters}',fontsize=16,fontweight = 'semibold')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/Silhouette_analysis_{n_clusters}.jpg')\n",
    "  \n",
    "    return stored_clusters, silh_avg_n_clusters, ax"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gxWzfws89Jg"
   },
   "source": [
    "#### Visualizing Silhouettes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "executionInfo": {
     "elapsed": 18663,
     "status": "ok",
     "timestamp": 1649738604182,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "EUUBKiJV-vTS",
    "outputId": "ddba2425-c871-4043-948b-e824a4643eb2",
    "scrolled": false
   },
   "source": [
    "stored_2, avg_2, ax_2 = find_opt_clusters_4(data, new_emb, 2, 7)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y-FW-BhzQQTl"
   },
   "source": [
    "##acroding to the silhouette analysis, num of clusters = 2 or 3, no clear clusters\n",
    "# cluster_data, cluster_id = clustering_question(data, 'emb', 4)\n",
    "\n",
    "'''\n",
    "It seems like clustering is not a good analysis technique for (this) paper dataset(s).\n",
    "Next step is to experiment with BERT-based topic modeling.\n",
    "'''\n",
    "\n",
    "#only select thw desired columns to save\n",
    "data = data[['title', 'abstract', 'doi', 'year', 'issue',\n",
    "       'citation_info', 'authors', 'author affiliation', 'pdf_link',\n",
    "       'issue_section', 'topics', 'cited times', 'keywords', 'reference', 'cluster_2', 'cluster_3',\n",
    "       'cluster_4', 'cluster_5', 'cluster_6']]\n",
    "\n",
    "data.to_excel('data with cluster.xlsx')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "executionInfo": {
     "elapsed": 1067,
     "status": "ok",
     "timestamp": 1649828247789,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "0cjjw-6UI1c-",
    "outputId": "6f2d7d1e-f72d-4710-fec5-53b711768fdd"
   },
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 10)\n",
    "ax.set_xlabel('Principle Component 1', fontsize=18)\n",
    "ax.set_ylabel('Principle Component 2', fontsize=18)\n",
    "# ax.title.set_text('2 Component PCA for 7 Clusters', fontsize=20)\n",
    "scatter = ax.scatter(data['X'], data['Y'], c=data['cluster_4'], cmap='rainbow')  #s = data['Frequency1']\n",
    "plt.xticks(rotation = 0, fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc = \"upper right\", fontsize=18) #, title=\"Clusters\"\n",
    "ax.add_artist(legend1)\n",
    "# plt.savefig('plots/topic_cluster {}.png'.format(n), dpi = 800, bbox_inches='tight')\n",
    "# plt.savefig('plots/author_cluster.png'.format(n), dpi = 800, bbox_inches='tight')\n",
    "plt.savefig('plots/paper_cluster 4.png'.format(n), dpi = 800, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fFAHHcCL7Tkd"
   },
   "source": [
    "from dash import Dash, dcc, html, Input, Output\n",
    "import plotly.express as px"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 22587,
     "status": "ok",
     "timestamp": 1649832433914,
     "user": {
      "displayName": "Sarah Zhao",
      "userId": "06677117688470630557"
     },
     "user_tz": 240
    },
    "id": "L0Wpe4u9rHsa",
    "outputId": "e9e10fe6-6b72-482f-8b85-7cd61897b971"
   },
   "source": [
    "##interactive 3d visualization\n",
    "import plotly.express as px\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(new_emb)\n",
    "x = pca.fit_transform(new_emb)\n",
    "\n",
    "data['X1'] = x[:,0]\n",
    "data['Y1'] = x[:,1]\n",
    "data['Z1'] = x[:,2]\n",
    "\n",
    "fig = px.scatter_3d(data, x='X1', y='Y1',z='Z1',\n",
    "                    color=\"cluster_4\")\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "og75_7aazYQW",
    "VgHvpM9czGv_",
    "VZdSWNPYzBwp",
    "ypze3OGGy9X_",
    "v5LcYhKf4WVv",
    "V84-F4RyL3y-",
    "zaG24OBnadAZ",
    "EVd3CQwkXEWx",
    "7gxWzfws89Jg",
    "poMkh3YuYzxL",
    "45rLbUGjKYn8",
    "NJow1keBZkPb",
    "SJ4VqE0aYkJm",
    "a3DVmgh7YnPr"
   ],
   "machine_shape": "hm",
   "name": "jmd_bert.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
