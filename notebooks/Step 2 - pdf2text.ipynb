{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d57471a8",
   "metadata": {},
   "source": [
    "# Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69df1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from nltk import ngrams\n",
    "import os\n",
    "import string\n",
    "\n",
    "'''\n",
    "self-defined pdf2text function: The \"parse\" funtions needs to be adapted according to specific pdf format of \n",
    "a certain jounal\n",
    "ref source: https://www.reddit.com/r/Python/comments/50r1cs/text_extraction_from_pdf_published_scientific/\n",
    "'''\n",
    "\n",
    "def convert_pdf_to_txt_and_metadata(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    metadata=PDFDocument(PDFParser(fp)).info\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    text = retstr.getvalue()\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text, metadata\n",
    "\n",
    "def findWord(word, pdftext):\n",
    "    for sentence in pdftext.split(\".\"):\n",
    "        if word in sentence:\n",
    "            print(sentence.replace(\"\\n\", \" \"))\n",
    "            \n",
    "def parse(text, content):        \n",
    "    pages = text.split('\\x0c')\n",
    "    remove_lst = ['', ' ', '  ', '   ']\n",
    "    pages = [item for item in pages if item not in remove_lst]\n",
    "    full = []\n",
    "    ##clean each page\n",
    "    for page in range(len(pages)):\n",
    "        if \"l\\n\\nD\\no\\nw\\nn\\no\\na\\nd\\ne\\nd\\n\\n\" in pages[page]:\n",
    "            txt1 = pages[page].split('l\\n\\nD\\no\\nw\\nn\\no\\na\\nd\\ne\\nd\\n\\n')[0]\n",
    "            txt2 = pages[page].split('l\\n\\nD\\no\\nw\\nn\\no\\na\\nd\\ne\\nd\\n\\n')[1].split('\\n2\\n0\\n2\\n2')[1]\n",
    "            txt = (txt1 + txt2)\n",
    "        else:\n",
    "            txt = pages[page]\n",
    "            \n",
    "        txt = txt.replace('-\\n\\n', '').split('\\n\\n')\n",
    "        txt = [item for item in txt if item not in remove_lst]\n",
    "        remove = []\n",
    "        for i in range(len(txt)):\n",
    "            length = len(txt[i])\n",
    "            \n",
    "            ##remove noisy items\n",
    "            str_len = len(''.join(e for e in txt[i] if e.isalnum() if not e.isdigit()))\n",
    "            if str_len <= 2 or (str_len < 0.6*length and length < 40):\n",
    "                remove += [txt[i]]\n",
    "                continue\n",
    "                \n",
    "            ##detect and remove noise (i.e., download links)\n",
    "            if str_len < 0.6*len(txt[i]):\n",
    "                txt[i] = txt[i].replace('-\\n', '').replace('\\n','').strip()\n",
    "            else:\n",
    "                txt[i] = txt[i].replace('-\\n', '').replace('\\n',' ').strip()\n",
    "            \n",
    "            ##remove journal logos items\n",
    "            for item in ['Journal of Mechanical Design',\"Transactions of the ASME\",'Vol.','Copyright © ',\n",
    "                         'Contributed','Corresponding author',\n",
    "                         'Journal of Mechanisms, Transmissions, and Automation in Design']:\n",
    "                if item in txt[i] and len(txt[i]) < 300:\n",
    "                    remove += [txt[i]]\n",
    "                    \n",
    "            ## remove journal logos contained in text\n",
    "            for item in ['Journal of Mechanical Design',\"Transactions of the ASME\",'Vol.','Copyright © ',\n",
    "                         'Journal of Mechanisms, Transmissions, and Automation in Design']:\n",
    "                if item in txt[i] and len(txt[i]) >= 300:\n",
    "                    txt[i] = txt[i].split(item)[0]\n",
    "                    \n",
    "            ## remove footnot regaring author info from text\n",
    "            for item in ['Contributed','Corresponding author']:\n",
    "                if item in txt[i] and len(txt[i]) >= 300 and len(txt[i].split(item)[1]) < 300:\n",
    "                    txt[i] = txt[i].split(item)[0]\n",
    "                    \n",
    "            ## detect paragraphs that are split into two parts by footnotes\n",
    "            if i > 0 and i < len(txt) - 1 and len(txt[i]) > 2:\n",
    "                if txt[i][0:2] in [str(a)+b for a in range(20) for b in list(string.ascii_uppercase)] \\\n",
    "                   and txt[i][-1] == '.':\n",
    "                    if txt[i-1][-1] not in ['.','?', '!'] and txt[i+1][0] not in list(string.ascii_uppercase):\n",
    "                        txt[i],txt[i+1] = txt[i+1],txt[i]\n",
    " \n",
    "        ##remove empty items\n",
    "        remove = set(remove)\n",
    "        for item in remove:\n",
    "            txt.remove(item)\n",
    "        ##insert \"@#\" between meta data for easy parsing\n",
    "        if content == 1:\n",
    "            tmpx = ' '.join(txt)\n",
    "        else:\n",
    "            tmpx = ' @# '.join(txt)\n",
    "        full += [tmpx]\n",
    "    \n",
    "    cleaned_text = ' '.join(full)\n",
    "    return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0319beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load the spreadsheet in which the sparsed info will be stored\n",
    "data = pd.read_excel('jmd_all_updated.xlsx')\n",
    "\n",
    "##define the dictionary for storing the extracted info\n",
    "info = {}\n",
    "info['ref'] = ['']*len(data)\n",
    "info['ack'] = ['']*len(data)\n",
    "info['meta'] = ['']*len(data)\n",
    "info['full_text'] = ['']*len(data)\n",
    "for no in range(0,len(data)):\n",
    "    pdf, metadata=convert_pdf_to_txt_and_metadata(os.path.join(\"folder_with_pdfs\", \"%d.pdf\"%no))\n",
    "    for i in range(20):\n",
    "        if i%2 == 1:\n",
    "            pdf = pdf.replace('(cid:%d)'%i, '(')\n",
    "        else:\n",
    "            pdf = pdf.replace('(cid:%d)'%i, ')')\n",
    "            \n",
    "    ##split pdf into different sections containing different info and extract the desired info using \"parse\"\n",
    "    ref = ''\n",
    "    for item in ['References\\n','References \\n','R\\ne\\nf\\ne\\nr\\ne\\nn\\nc\\ne\\ns','Reference\\n','Reference \\n',\n",
    "                 'R\\ne\\nf\\ne\\nr\\ne\\nn\\nc\\ne']:\n",
    "        if item in pdf:\n",
    "            ##split on last occurrence\n",
    "            REF = pdf.rsplit(item,1)[-1] \n",
    "            pdf = pdf.rsplit(item,1)[-2]\n",
    "#             ref = parse(REF,2)\n",
    "            break\n",
    "    \n",
    "    ack = ''\n",
    "    for item in ['Acknowledgments\\n','Acknowledgments \\n','A\\nc\\nk\\nn\\no\\nw\\nl\\ne\\nd\\ng\\nm\\ne\\nn\\nt\\ns',\n",
    "                 'Acknowledgment\\n','Acknowledgment \\n','A\\nc\\nk\\nn\\no\\nw\\nl\\ne\\nd\\ng\\nm\\ne\\nn\\nt']:\n",
    "        if item in pdf:\n",
    "            ##split on last occurrence\n",
    "            ACK = pdf.rsplit(item,1)[-1]\n",
    "            pdf = pdf.rsplit(item,1)[-2]\n",
    "            ack = parse(ACK,1)\n",
    "            break\n",
    "    \n",
    "    meta = ''\n",
    "    for item in ['DOI: ',\"Introduction \\n\",'Introduction\\n','I\\nn\\nt\\nr\\no\\nd\\nu\\nc\\nt\\ni\\no\\nn\\n',\n",
    "                 'I\\nn\\nt\\nr\\no\\nd\\nu\\nc\\nt\\ni\\no\\nn \\n']:\n",
    "        if item in pdf:\n",
    "            ##split on first occurrence\n",
    "            META = pdf.split(item,1)[0]\n",
    "            meta = parse(META,2)\n",
    "            if item == 'DOI: ':\n",
    "                pdf = pdf.split(item,1)[1][20:]\n",
    "            else: \n",
    "                pdf = 'Introduction\\n' + pdf.split(item,1)[-1]\n",
    "            break\n",
    "        \n",
    "    text = parse(pdf,1)\n",
    "    \n",
    "    info['ref'][no] = ref\n",
    "    info['ack'][no] = ack\n",
    "    info['meta'][no] = meta\n",
    "    info['full_text'][no] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "008c17ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##capture second-column text in first page going to meta incorretly\n",
    "n = 0\n",
    "data = data.astype({\"abstract\": str})\n",
    "abstract = list(data['abstract'])\n",
    "meta1 = ['']*len(data)\n",
    "for i in range(len(info['meta'])):\n",
    "# for i in [8]:\n",
    "    meta = info['meta'][i]\n",
    "    abst = abstract[i]\n",
    "    if len(abst) > 10 and abst[-8:]+' @# ' in meta:\n",
    "        meta1[i] = meta.split(abst[-8:]+' @# ')[-1].replace(' @# ', ' ')\n",
    "        \n",
    "##correct main body text by adding \n",
    "corrected = ['']*len(data)\n",
    "\n",
    "for i in range(len(info['full_text'])):\n",
    "    corrected[i] = meta1[i].strip() + ' ' + info['full_text'][i].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1778a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The maximal cell length of excel is 32767. Need to cut long text into multiple pieces\n",
    "body = {}\n",
    "m = 0\n",
    "for item in info['full_text']:\n",
    "    n = int(len(item) / 32767) + 1\n",
    "    for i in range(n):\n",
    "        if 'text_' + str(i) not in body.keys():\n",
    "            body['text_' + str(i)] = ['']*len(data)\n",
    "        body['text_' + str(i)][m] = item[i*32767:min((i+1)*32767, len(item))]\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d694704",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ref from pdf'] = info['ref']\n",
    "data['acknowledgment'] = info['ack']\n",
    "data['main body'] = info['full_text']\n",
    "data['meta data'] = info['meta']\n",
    "data['meta to main body'] = meta1\n",
    "data['correccted main body'] = corrected\n",
    "for key in body.keys():\n",
    "    data[key] = body[key]\n",
    "\n",
    "data.to_excel('jmd_all_updated.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d27e82",
   "metadata": {},
   "source": [
    "# Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bd3cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Approach 1: not generic, less effective for pdfs of earlier-year papers\n",
    "Approach 2: generic to all journals, but can't be applied to all pdfs successfully. A little time-consuming \n",
    "to install the packages. Run well on Linux (Ubuntu), but not on Windows.\n",
    "For low-quality pdfs, approach 1 catch paper content more comprehensively, but it's less accurate at the word level.\n",
    "Many words have extra spaces between letters.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b7f3c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ref source 1: https://github.com/titipata/scipdf_parser\n",
    "    pip install git+https://github.com/titipata/scipdf_parser\n",
    "    ##The following command needs to be run everytime when the script is in use.\n",
    "    ##The following command needs to run on Linux; the file needs to be revised according to ref 2.\n",
    "    ##Go to the folder containing the bash file in terminal, then run:\n",
    "    bash serve_grobid.sh\n",
    "ref source 2: https://github.com/kermitt2/grobid\n",
    "'''\n",
    "import scipdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a148dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##test\n",
    "article_dict = scipdf.parse_pdf_to_dict('folder_with_pdfs/131.pdf') # return dictionary\n",
    " \n",
    "## option to parse directly from URL to PDF, if as_list is set to True, output 'text' of parsed section will be in a list of paragraphs instead\n",
    "# article_dict = scipdf.parse_pdf_to_dict('https://www.biorxiv.org/content/biorxiv/early/2018/11/20/463760.full.pdf', as_list=False)\n",
    "\n",
    "xml = scipdf.parse_pdf('folder_with_pdfs/131.pdf', soup=True) # option to parse full XML from GROBID"
   ]
  },
  {
   "cell_type": "code",
   "id": "2f1f9b18",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data = pd.read_excel('jmd_all_updated 1.xlsx')\n",
    "\n",
    "body = ['']*len(data)\n",
    "acknow = ['']*len(data)\n",
    "path = 'folder_with_pdfs'\n",
    "for i in range(0,5591):\n",
    "    text = []\n",
    "    ack = ''\n",
    "    try:\n",
    "        article_dict = scipdf.parse_pdf_to_dict(os.path.join(path,str(i)+'.pdf'))\n",
    "        for sec in article_dict['sections']:\n",
    "            head = sec['heading'].replace('\\n','').strip()\n",
    "            if \"Acknowledgment\" not in head and \"acknowledgment\" not in head:\n",
    "                txt = sec['text'].replace('\\n','').strip()\n",
    "                text += [head + ' ' + txt]\n",
    "            else:\n",
    "                ack = sec['text'].replace('\\n','').strip()\n",
    "    except:\n",
    "        print (\"Doesn't work for paper \" + str(i))\n",
    "    body_text = ' '.join(text)\n",
    "\n",
    "    body[i] = body_text\n",
    "    acknow[i] = ack"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "2b108f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_body = {}\n",
    "m = 0\n",
    "for item in body:\n",
    "    n = int(len(item) / 32767) + 1\n",
    "    for i in range(0,n):\n",
    "        if 'text_'+str(i) not in main_body.keys():\n",
    "            main_body['text_'+str(i)] = ['']*len(data)\n",
    "        main_body['text_'+str(i)][m] = item[i*32767:min((i+1)*32767,len(item))]\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "d31c29cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in main_body.keys():\n",
    "    data[key] = main_body[key]\n",
    "data['Ack'] = acknow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "eee0fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('jmd_all_updated.xlsx')\n",
    "\n",
    "'''\n",
    "Manual work is needed to check, revise, combine the extracted texts.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "210682af",
   "metadata": {},
   "outputs": [],
   "source": [
    "##extract figures from pdf files\n",
    "##issue: can only run for 20s (~6 papers). Need to split papers into multiple folders woth a smaller number of pdfs\n",
    "\n",
    "##folder containing pdfs: folder should contain only PDF files\n",
    "path_in = \"folder_with_pdfs\"\n",
    "##folder containing figures\n",
    "path_out = \"figures\"\n",
    "scipdf.parse_figures(path_in, output_folder=path_out)\n",
    "scipdf.parse_figures(input_folder, output_folder=path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usaid_processing",
   "language": "python",
   "name": "usaid_processing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
