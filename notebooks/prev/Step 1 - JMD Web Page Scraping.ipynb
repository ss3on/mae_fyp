{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T06:26:38.024933Z",
     "start_time": "2025-07-27T06:26:38.022929Z"
    }
   },
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Apr 2 13:20:39 2022\n",
    "Updated on Mon Jun 20 20:11:48 2022\n",
    "\n",
    "@author: Binyang\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "from importlib import reload\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T06:26:39.279490Z",
     "start_time": "2025-07-27T06:26:39.276146Z"
    }
   },
   "source": [
    "def sleep(duration):\n",
    "    print(f'sleep {duration}s')\n",
    "    time.sleep(duration)\n",
    "\n",
    "def parse(text, parser='html.parser'):\n",
    "    return BeautifulSoup(text, parser)\n",
    "\n",
    "def fetch(url, file_path=None, session=None, max_attempts=10, custom_headers=None):\n",
    "    if custom_headers is None:\n",
    "        custom_headers = {}\n",
    "\n",
    "    if file_path is not None:\n",
    "        print(f'Fetch {url} to {file_path}')\n",
    "        if Path(file_path).exists():\n",
    "            print('Already exists. Skipped.')\n",
    "            return None\n",
    "    else:\n",
    "        print(f'Fetch {url}')\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "        if session is not None:\n",
    "            response = session.get(url, headers=custom_headers)\n",
    "        else:\n",
    "            response = get(url, headers=custom_headers)\n",
    "\n",
    "        # handle HTTP 429 Too Many Requests\n",
    "        if response.status_code == 429:\n",
    "            delay = (attempt + 1) * 10\n",
    "            print(f'Retrying in {delay} seconds')\n",
    "            time.sleep(delay)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    response.raise_for_status()\n",
    "\n",
    "    if session is not None:\n",
    "        print(f'Session cookies: {len(session.cookies)}')\n",
    "\n",
    "    if file_path is not None:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "    return response.text"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T11:00:13.745138Z",
     "start_time": "2025-07-27T11:00:13.561574Z"
    }
   },
   "source": [
    "##data_scraping\n",
    "custom_headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "\n",
    "doc = parse(fetch(\"https://asmedigitalcollection.asme.org/mechanicaldesign/issue/browse-by-year\", custom_headers=custom_headers))"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m##test\u001B[39;00m\n\u001B[32m      2\u001B[39m custom_headers = {\u001B[33m'\u001B[39m\u001B[33mUser-Agent\u001B[39m\u001B[33m'\u001B[39m: \u001B[33m'\u001B[39m\u001B[33mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\u001B[39m\u001B[33m'\u001B[39m}\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m doc = \u001B[43mparse\u001B[49m(fetch(\u001B[33m\"\u001B[39m\u001B[33mhttps://asmedigitalcollection.asme.org/mechanicaldesign/issue/browse-by-year\u001B[39m\u001B[33m\"\u001B[39m, custom_headers=custom_headers))\n",
      "\u001B[31mNameError\u001B[39m: name 'parse' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "all_docs_info = []\n",
    "main_url = \"https://asmedigitalcollection.asme.org\"\n",
    "columns_to_take = [\"title\", \"abstract\", \"doi\", \"year\", \"issue\", \"citation_info\", \"authors\", \"pdf_link\", \"issue_section\",\n",
    "                   \"topics\", \"keywords\", \"related article\", \"source of related articles\", \"cited times\", \n",
    "                   \"author affiliation\", \"reference\"]\n",
    "doc = parse(fetch(\"https://asmedigitalcollection.asme.org/mechanicaldesign/issue/browse-by-year\", custom_headers=custom_headers))\n",
    "##get the links to the webpage of each year\n",
    "for year_link in doc.select(\"div.widget-ContentBrowseAllYearsManifest a\"):\n",
    "    year = int(year_link.text.strip())\n",
    "\n",
    "    ##scrape JMD website by year\n",
    "    if(year == 2022):\n",
    "        year_href = year_link[\"href\"]\n",
    "        year_doc = parse(fetch(year_href, custom_headers=custom_headers))\n",
    "        \n",
    "        ##scrape all issues in a certain year\n",
    "        for issue_link in year_doc.select(\"div.widget-ContentBrowseByYearManifest a\"):\n",
    "            issue_text = issue_link.text.strip()\n",
    "            issue_link  = main_url + issue_link[\"href\"]\n",
    "            print(year, issue_link)\n",
    "            issue_doc = parse(fetch(issue_link, custom_headers=custom_headers))\n",
    "            \n",
    "            ##get the link to each paper in a certain issue\n",
    "            ##scrape the desired data from paper webpage\n",
    "            for article in issue_doc.select(\"div.al-article-item-wrap\"):\n",
    "                article_obj = {}\n",
    "                for column in columns_to_take:\n",
    "                    article_obj[column] = \"\"\n",
    "                article_obj[\"year\"] = year\n",
    "                article_obj[\"issue\"] = issue_text\n",
    "                ##title\n",
    "                if article.select(\"h5.item-title\"):\n",
    "                    article_obj[\"title\"] = article.select(\"h5.item-title\")[0].text.strip()\n",
    "                ##author\n",
    "                article_obj[\"authors\"] = \";\".join([author.text.strip() for author in article.select(\"div.al-authors-list\")])\n",
    "                ##pdf link\n",
    "                if article.select(\"a.article-pdfLink\"):\n",
    "                    article_obj[\"pdf_link\"] = main_url + article.select(\"a.article-pdfLink\")[0][\"href\"]\n",
    "                ##other info\n",
    "                if article.select(\"h5.item-title a\"):\n",
    "                    article_link = main_url + article.select(\"h5.item-title a\")[0][\"href\"]\n",
    "                    article_doc = parse(fetch(article_link, custom_headers=custom_headers))\n",
    "                    ##abstract\n",
    "                    article_obj[\"abstract\"] = \"\\n\".join([abstract_section.text for abstract_section in article_doc.select(\".abstract\")])\n",
    "                    ##cite_info\n",
    "                    article_obj[\"citation_info\"] = \";\".join([section.text.strip() for section in article_doc.select(\".ww-citation-primary\")])\n",
    "                    ##doi\n",
    "                    if article_doc.select(\"a.ww-doi-link\"):\n",
    "                        article_obj[\"doi\"] = article_doc.select(\"a.ww-doi-link\")[0][\"href\"]\n",
    "                    ##issue_section\n",
    "                    article_obj[\"issue_section\"] = \" \".join([phrase.text for phrase in article_doc.select(\".content-metadata-tocSections\")]).strip()\n",
    "                    ##topic\n",
    "                    article_obj[\"topics\"] = \" \".join([phrase.text for phrase in article_doc.select(\".content-metadata-topics\")]).strip()\n",
    "                    ##keywords\n",
    "                    article_obj[\"keywords\"] = \" \".join([phrase.text for phrase in article_doc.select(\".content-metadata-keywords\")]).strip()\n",
    "                    ##most related ASME papers assigned by ASME \n",
    "                    article_obj[\"related article\"] = \";\".join([related_articles.text for related_articles in article_doc.select(\"div.article-title a\")]).strip()\n",
    "                    article_obj[\"source of related articles\"] = \";\".join([sources.text for sources in article_doc.select(\"div.content-meta\")]).strip()\n",
    "                    ##times being cited\n",
    "                    article_obj[\"cited times\"] = \";\".join([web.text for web in article_doc.select(\"div.article-cited-link-wrap a\")]).strip()\n",
    "                    ##author affiliations\n",
    "                    aff0 = ''\n",
    "                    for affiliation in article_doc.select(\"div.author-affiliation\"):\n",
    "                        for aff in affiliation.select(\"div.aff\"):\n",
    "                            aff0 = \";\".join([aff0,aff.text])\n",
    "                    article_obj[\"author affiliation\"] = aff0.strip()[1:]\n",
    "                    ##references of a focal paper\n",
    "                    if article_doc.select(\"div.ref-list\"):\n",
    "                        reference = ''\n",
    "                        doix = ''\n",
    "                        sourcex = ''\n",
    "                        titlex = ''\n",
    "                        for ref in article_doc.select(\"div.ref-list\")[0].select('div.citation'):\n",
    "                            if ref.select('div.article-title'):\n",
    "                                titlex = \"title::\" + ref.select('div.article-title')[0].text.strip()\n",
    "                        #     print (ref.text.strip())\n",
    "                            if ref.select('div.source'):\n",
    "                                sourcex = \"source::\" + ref.select('div.source')[0].text.strip()\n",
    "                            if ref.select('div.crossref-doi a'):\n",
    "                                doix = \"doi::\" + ref.select('div.crossref-doi a')[0]['href'].replace('dx.', '')\n",
    "                            referencex = ';'.join([titlex, sourcex, doix])\n",
    "                            reference = '||'.join([reference, referencex])\n",
    "                        article_obj[\"reference\"] = reference[2:]\n",
    "                print(article_obj[\"title\"])\n",
    "                all_docs_info.append(article_obj)\n",
    "\n",
    "print(\"Number of docs: \", len(all_docs_info))\n",
    "\n",
    "df_with_articles = pd.DataFrame(all_docs_info)\n",
    "\n",
    "df_with_articles.to_excel(\"jmd_all_updated.xlsx\", encoding=\"utf-8\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# import urllib\n",
    "# def download_pdf_by_link(download_url, filename):\n",
    "#     try:\n",
    "#         response = urllib.request.urlopen(download_url)\n",
    "#         pdf_data = response.read()\n",
    "#         assert len(pdf_data) > 5000\n",
    "#         file = open(filename, 'wb')\n",
    "#         file.write(pdf_data)\n",
    "#         file.close()\n",
    "#         print(\"Downloaded file \", filename)\n",
    "#     except Exception as err:\n",
    "#         print(err)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "custom_headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def download_pdf_by_link(download_url, filename):\n",
    "    try:\n",
    "        r = requests.get(download_url, headers = custom_headers)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print(\"Downloaded file \", filename)\n",
    "    except Exception as err:\n",
    "        print(err)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import os\n",
    "import time\n",
    "# os.makedirs(\"folder_with_pdfs\", exist_ok=True)\n",
    "\n",
    "df_with_articles = pd.read_excel('jmd_all_updated.xlsx')\n",
    "n = 0\n",
    "for i in range(0,len(df_with_articles)):\n",
    "    print(df_with_articles[\"pdf_link\"].values[i])\n",
    "    if df_with_articles[\"pdf_link\"].values[i].strip():\n",
    "        download_pdf_by_link(df_with_articles[\"pdf_link\"].values[i], os.path.join(\"folder_with_pdfs\", \"%d.pdf\"%i))\n",
    "        n += 1\n",
    "        if n%15 == 0:\n",
    "            time.sleep(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "##detect cases that multiple papers share the same pdf file \n",
    "for i in range(1,len(df_with_articles)):\n",
    "# for i in range(1133,1134):\n",
    "    size = os.path.getsize(os.path.join(\"folder_with_pdfs\", \"%d.pdf\"%i)) \n",
    "    size1 = os.path.getsize(os.path.join(\"folder_with_pdfs\", \"%d.pdf\"%(i-1))) \n",
    "    if size == size1:\n",
    "        print (i)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usaid_processing",
   "language": "python",
   "name": "usaid_processing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
